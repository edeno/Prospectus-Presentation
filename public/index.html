<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Eric Denovellis">
  <title>The role of prefrontal cortex in task switching</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="revealjs/css/reveal.css"/>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="revealjs/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="custom.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'revealjs/css/print/pdf.css' : 'revealjs/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="revealjs/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">The role of prefrontal cortex in task switching</h1>
    <h2 class="author">Eric Denovellis</h2>
    <h3 class="date"></h3>
</section>

<section id="monsell" class="slide level1" data-background="img/apple-iphone.jpg">
<h1></h1>
<audio data-autoplay>
<source data-src="cellphoneVibrate.mp3" type="audio/mp3" />
</audio>
<blockquote>
‚ÄúEach of the objects and events we encounter in the environment affords a range of possible actions in response to it. The <strong>appropriate response</strong> varies as a function of the <strong>task</strong>‚Äù
<footer>
‚Äî Rogers &amp; Monsell (1995)
</footer>
</blockquote>
<aside class="notes">
<p>Contrast + Answering phone in your office</p>
<ul>
<li>Answering phone in this talk</li>
</ul>
</aside>
</section>
<section id="rule-task-set" class="slide level1">
<h1>Rule / Task Set</h1>
<figure>
<img src="img/goals-stimuli-response.svg" alt="" />
</figure>
<aside class="notes">
<ul>
<li><p>conceptual boundary for what constitutes distinct tasks fuzzy</p></li>
<li><p>Can adopt rules / task sets before knowing what environmental stimuli there are</p></li>
<li>An environmental stimuli might have a strong response associated with it, but we can pick a weaker, more appropriate response</li>
</ul>
</aside>
</section>
<section id="cognitive-demand-the-monitoring-and-allocation-of-control" class="slide level1">
<h1>Cognitive Demand: The monitoring and allocation of control</h1>
<p>Adjustments in control can occur rapidly. Reaction time and accuracy tend to change:</p>
<ul>
<li><p>After errors</p></li>
<li><p>Repeating the same task</p></li>
<li><p>When switching between tasks</p></li>
<li><p>When responding to stimuli with more than response associated with them</p></li>
</ul>
</section>
<section id="section" class="slide level1" data-background="white">
<h1></h1>
<figure>
<img src="img/Shallice.jpg" alt="" />
</figure>
<aside class="notes">
<ul>
<li><p>need to configure appropriate set of processes linking sensory to motor (categorization, mapping, response)</p></li>
<li><p>Don‚Äôt always need to control (continuum between controlled vs.¬†automatic processing)</p></li>
<li><p>Fluctuations in control</p></li>
<li><p>limited capacity for control</p></li>
<li><p>people adjust their allocation of control as circumstances demand</p></li>
<li><p>Stroop task: When responding to stimuli with more than response associated with them (congruency)</p></li>
<li>Interestingly, effect isn‚Äôt as strong when there are more incongruent trials relative to congruent trials</li>
</ul>
</aside>
</section>
<section id="prefrontal-cortex-is-important-for-monitoring-and-allocation-of-control" class="slide level1">
<h1>Prefrontal Cortex is important for monitoring and allocation of control</h1>
<ul>
<li><p>Perseverative behavior after injury</p></li>
<li><p>Neurons that respond selectivity to one rule</p></li>
</ul>
</section>
<section id="dynamics-how-is-activity-on-network-level-in-pfc-coordinated-so-that-the-appropriate-response-is-selected" class="slide level1">
<h1>Dynamics: How is activity on network level in PFC coordinated so that the appropriate response is selected?</h1>
<ul>
<li><p>PFC neurons need to be able flexibly represent and switch between rules to bias other areas toward the appropriate sensory-motor representations.</p></li>
<li><p>They need to suppress inappropriate sensory-motor representations.</p></li>
</ul>
</section>
<section id="functional-organization-of-the-prefrontal-cortex" class="slide level1">
<h1>Functional Organization of the Prefrontal Cortex</h1>
<figure>
<img src="img/rule-selectivity-pfc.svg" alt="" />
</figure>
<aside class="notes">
<p>Rule selective neurons have been found in different subdivisions of PFC</p>
Not much is known about how this representation interacts with attentionally demanding factors.
</aside>
</section>
<section id="the-problem-of-the-acc-no-consensus" class="slide level1">
<h1>The problem of the ACC ‚Äî No consensus</h1>
<ul>
<li>Selection for action?</li>
<li>Error Detection?</li>
<li>Conflict Monitoring?</li>
<li>Reinforcement Learning?</li>
<li>Error Likelihood?</li>
<li>Predicted Response-Outcome?</li>
<li>Hierarchical Error Representation?</li>
</ul>
<aside class="notes">
<p>Theory #1: Selection-For-Action ‚Äì The ACC is responsible for selecting the relevant sensory information to make an action. It prevents competition between possible responses. Factors predicted: Rule, Test Stimulus, Response Direction.</p>
<p>Theory #2: Error Detection ‚Äì The ACC makes a comparison between the response in progress and the intended response (the correct response) or between the received reward and expected reward. Can also use that comparison to adjust or correct responses even as the response is ongoing. Factors predicted: Previous Error, Response Direction.</p>
<p>Theory #3: Conflict Monitoring ‚Äì The ACC detects incompatible representations (responses, stimuli, contexts, etc.) and signals lateral prefrontal areas to make attentional adjustments. Factors predicted: Switch, Previous Error, Congruency.</p>
<p>Theory #4: Response-Outcome Associations/Reinforcement Learning ‚Äì The ACC learns to associate responses with an expected outcome (positive or negative) and the amount of effort required to achieve that outcome (cost and frequency of response, action-reward ratios) ‚Äì Pr(Outcome | Response). In the context of cued task switching, the ACC‚Äôs role should be less central, since response and reward combinations are not particularly informative. Factors predicted: Previous Error, Response Direction.</p>
<p>Theory #5: Error Likelihood ‚Äì ACC learns to associate error with the active stimulus-response representation that caused the error (through a self-organizing map trained by a dopamine teaching signal), meaning ACC detects the situations in which an error is more likely. Factors predicted: Rule, Switch, Previous Error, Test Stimulus, Response Direction.</p>
<p>Theory #6: Predicted Response-Outcome: The ACC learns to associate predictions of responses and their outcomes given a stimulus context ‚ÄìPr(Response, Outcome | Context). ACC signals when an unexpected non-occurrence of a predicted outcome occurs (negative surprise). Factors predicted: Rule, Previous Error, Response Direction, and possibly Switch, Congruency</p>
Theory #7: Multidimensional error signals generated by ACC are used to train error prediction representations in dlPFC, and, once trained, error representations maintained by dlPFC are a basis of working memory signals that suffice to guide task performance. Specifically, the dlPFC learns to maintain representations of stimuli that reliably co-occur with prediction errors (Friston, 2010). In turn, the dlPFC error prediction representations are deployed by ACC to refine predictions about the likely outcomes of actions
</aside>
</section>
<section id="section-1" class="slide level1" data-background="white">
<h1></h1>
<figure>
<img src="img/mixed-selectivity.svg" alt="" />
</figure>
<aside class="notes">
<ul>
<li><p>Prefrontal neurons are known to integrate multiple parameters of a task (large receptive fields, lots of converging inputs, pyramidal neurons are more spiny)</p></li>
<li>Examining their effects individually might overestimate their importance</li>
</ul>
</aside>
</section>
<section id="outline" class="slide level1">
<h1>Outline</h1>
<ul>
<li><p>Task Switching Paradigm</p></li>
<li><p>Aim #1: LFP Dynamics</p></li>
<li><p>Aim #2: Single Neuron and Population Analysis</p></li>
<li><p>NEW Aim #3: Interactive visualization tools for multi-dimensional investigation of electrophyisological datasets</p></li>
</ul>
</section>
<section id="task-switching-paradigm" class="slide level1">
<h1>Task Switching Paradigm</h1>
<figure>
<img src="img/task-switch-paradigm-1.svg" alt="" />
</figure>
</section>
<section id="task-switching-paradigm-1" class="slide level1">
<h1>Task Switching Paradigm</h1>
<figure>
<img src="img/task-switch-paradigm-2.svg" alt="" />
</figure>
</section>
<section id="task-switching-paradigm-2" class="slide level1">
<h1>Task Switching Paradigm</h1>
<figure>
<img src="img/task-switch-paradigm-3.svg" alt="" />
</figure>
</section>
<section id="task-switching-paradigm-3" class="slide level1">
<h1>Task Switching Paradigm</h1>
<figure>
<img src="img/task-switch-paradigm.svg" alt="" />
</figure>
</section>
<section id="factors-that-aÔ¨Äect-the-amount-of-attention-needed-reaction-time" class="slide level1">
<h1>Factors that AÔ¨Äect the Amount of Attention Needed (Reaction Time)</h1>
<figure>
<img src="img/behavior-react-change-1.svg" alt="" />
</figure>
</section>
<section id="factors-that-aÔ¨Äect-the-amount-of-attention-needed-reaction-time-1" class="slide level1">
<h1>Factors that AÔ¨Äect the Amount of Attention Needed (Reaction Time)</h1>
<figure>
<img src="img/behavior-react-change-2.svg" alt="" />
</figure>
</section>
<section id="factors-that-aÔ¨Äect-the-amount-of-attention-needed-reaction-time-2" class="slide level1">
<h1>Factors that AÔ¨Äect the Amount of Attention Needed (Reaction Time)</h1>
<figure>
<img src="img/behavior-react-change-3.svg" alt="" />
</figure>
</section>
<section id="all-factors-that-aÔ¨Äect-behavior-reaction-time" class="slide level1">
<h1>All Factors that AÔ¨Äect Behavior (Reaction Time)</h1>
<figure>
<img src="img/behavior-react-change.svg" alt="" />
</figure>
</section>
<section id="aim-1" class="slide level1">
<h1>Aim #1</h1>
</section>
<section id="rule-dependent-networks-in-dlpfc" class="slide level1" data-background="white">
<h1>Rule-dependent networks in dlPFC</h1>
<figure>
<img src="img/alpha-beta-synchrony-1.png" alt="" />
</figure>
</section>
<section id="rule-dependent-networks-in-dlpfc-1" class="slide level1" data-background="white">
<h1>Rule-dependent networks in dlPFC</h1>
<figure>
<img src="img/alpha-beta-synchrony-2.png" alt="" />
</figure>
</section>
<section id="rule-dependent-networks-in-dlpfc-2" class="slide level1" data-background="white">
<h1>Rule-dependent networks in dlPFC</h1>
<figure>
<img src="img/alpha-beta-synchrony-3.png" alt="" />
</figure>
</section>
<section id="rule-dependent-networks-in-dlpfc-3" class="slide level1" data-background="white">
<h1>Rule-dependent networks in dlPFC</h1>
<figure>
<img src="img/alpha-beta-synchrony-4.png" alt="" />
</figure>
</section>
<section id="rule-dependent-networks-in-dlpfc-4" class="slide level1" data-background="white">
<h1>Rule-dependent networks in dlPFC</h1>
<figure>
<img src="img/alpha-beta-synchrony-5.png" alt="" />
</figure>
</section>
<section id="analysis-of-acc-and-dlpfc-acc-synchrony" class="slide level1" data-background="white">
<h1>Analysis of ACC and dlPFC-ACC Synchrony üëé</h1>
<figure>
<img src="img/cc4_Elec1_Elec8_Color-Orient_CohDiff.png" alt="Representative sample of dlPFC-ACC differences in rule coherence" /><figcaption>Representative sample of dlPFC-ACC differences in rule coherence</figcaption>
</figure>
</section>
<section id="analysis-of-acc-and-dlpfc-acc-synchrony-1" class="slide level1" data-background="white">
<h1>Analysis of ACC and dlPFC-ACC Synchrony üëé</h1>
<figure>
<img src="img/cc4_Elec9_Elec13_Color-Orient_CohDiff.png" alt="Representative sample of ACC-ACC differences in rule coherence" /><figcaption>Representative sample of ACC-ACC differences in rule coherence</figcaption>
</figure>
</section>
<section id="analysis-of-acc-and-dlpfc-acc-synchrony-2" class="slide level1" data-background="white">
<h1>Analysis of ACC and dlPFC-ACC Synchrony üëé</h1>
<figure>
<img src="img/cc4_Elec9_Elec13_Switch-Rep_Coh%20Sig-Only%20Diff.png" alt="There was no evidence of modulation by attentional factors either" /><figcaption>There was no evidence of modulation by attentional factors either</figcaption>
</figure>
</section>
<section id="aim-2" class="slide level1">
<h1>Aim #2</h1>
<aside class="notes">
<p>Motivation for Aim #2 and Aim #3</p>
<p>Our goal is to characterize the cellular-level contributions of ACC and dlPFC neurons and their relationship to the circuit-level activity. See if they can tell us something different from the LFP analysis.</p>
<p>Also use multi-dimensional generalized linear regression models to account for mixed selectivity.</p>
</aside>
</section>
<section id="demonstration-that-models-work" class="slide level1" data-background="white">
<h1>Demonstration that models work</h1>
<figure>
<img src="img/isa16-13-2-ACC-PreviousErrorResponseDirection.svg" alt="" />
</figure>
</section>
<section id="sfn-poster-analysis" class="slide level1" data-background="white">
<h1>SFN Poster Analysis</h1>
<figure>
<img src="img/FactorImportance.svg" alt="" />
</figure>
</section>
<section id="sfn-poster-problems" class="slide level1">
<h1>SFN Poster Problems</h1>
<ul>
<li><p>Many low firing rate neurons (75% of neurons &lt; 10 Hz)</p></li>
<li><p>Approach too data-mining centric</p></li>
<li><p>Not enough analysis of effect size</p></li>
<li><p>Smoothing over time with splines inconsistent</p></li>
<li><p>Solution: <strong>Regularization</strong></p></li>
</ul>
</section>
<section id="proposed-corrections-at-qualifying-exam" class="slide level1">
<h1>Proposed Corrections at Qualifying Exam</h1>
<ul>
<li><p>Fit one big model and examine effects</p></li>
<li><p>Use regularization as a soft-threshold to constrain the fitting space</p></li>
</ul>
</section>
<section id="problem-interpretability" class="slide level1" data-background="white">
<h1>Problem: Interpretability</h1>
<figure>
<img src="img/rule-preference-interpretability.svg" alt="" />
</figure>
</section>
<section id="current-status-constrained-set-of-models" class="slide level1">
<h1>Current Status: Constrained set of models</h1>
<ul>
<li><p>Previous Error + Response Direction</p></li>
<li><p>Previous Error + Response Direction + Rule Repetition</p></li>
<li><p>Previous Error + Response Direction + Congruency</p></li>
<li><p>Previous Error + Response Direction + Rule Repetition + Rule</p></li>
<li><p>Previous Error + Response Direction + Congruency + Rule</p></li>
</ul>
</section>
<section id="potential-problems" class="slide level1" data-background="white">
<h1>Potential Problems</h1>
<figure>
<img src="img/miller-rule.png" alt="Wallis et al. 2001 (in monkey PFC)" /><figcaption>Wallis et al. 2001 (in monkey PFC)</figcaption>
</figure>
<aside class="notes">
Maybe it‚Äôs not mixed selectivity. Maybe we just don‚Äôt have a task that discriminates enough between receptive fields?
</aside>
</section>
<section id="potential-problems-1" class="slide level1" data-background="white">
<h1>Potential Problems</h1>
<figure>
<img src="img/churchland.jpg" alt="Raposo et al. 2014 (in rat PPC)" /><figcaption>Raposo et al. 2014 (in rat PPC)</figcaption>
</figure>
</section>
<section id="potential-problems-2" class="slide level1" data-background="white">
<h1>Potential Problems</h1>
<figure>
<img src="img/pillow.jpg" alt="Park et al. 2014 (in monkey LIP)" /><figcaption>Park et al. 2014 (in monkey LIP)</figcaption>
</figure>
</section>
<section id="aim-3" class="slide level1">
<h1>Aim #3</h1>
<figure>
<img src="img/rasterVisDemo.gif" alt="RasterVis" /><figcaption><a href="https://github.com/edeno/RasterVis">RasterVis</a></figcaption>
</figure>
<figure>
<img src="img/SpectraVisDemo.gif" alt="SpectraVis" /><figcaption><a href="https://github.com/edeno/SpectraVis">SpectraVis</a></figcaption>
</figure>
<figure>
<img src="img/glmVisDemo.gif" alt="glmVis" /><figcaption><a href="https://github.com/edeno/glmVis">glmVis</a></figcaption>
</figure>
</section>
<section id="why-statistical-summaries-can-be-misleading-visualizations-help-us-check-our-assumptions" class="slide level1">
<h1>WHY: Statistical summaries can be misleading <span class="parenthetical">(Visualizations help us check our assumptions)</span></h1>
<figure>
<img src="img/Anscombe&#39;s_quartet_3.svg" alt="Anscombe‚Äôs quartet" /><figcaption>Anscombe‚Äôs quartet</figcaption>
</figure>
</section>
<section id="why-we-are-collecting-more-data" class="slide level1">
<h1>WHY: We are collecting more data</h1>
<ul>
<li>More data ‚Üí more complex theories ‚Üí more comparisons</li>
</ul>
</section>
<section id="why-we-are-collecting-more-data-1" class="slide level1">
<h1>WHY: We are collecting more data</h1>
<ul>
<li><p>More data ‚Üí more complex theories ‚Üí more comparisons</p></li>
<li><p>Multidimensional data requires multiple views and summaries to fully understand</p></li>
</ul>
</section>
<section id="why-we-are-collecting-more-data-2" class="slide level1">
<h1>WHY: We are collecting more data</h1>
<ul>
<li><p>More data ‚Üí more complex theories ‚Üí more comparisons</p></li>
<li><p>Multidimensional data requires multiple views and summaries to fully understand</p></li>
<li><p>Harder to display all the data on a single static figure</p></li>
</ul>
</section>
<section id="why-we-are-collecting-more-data-3" class="slide level1">
<h1>WHY: We are collecting more data</h1>
<ul>
<li><p>More data ‚Üí more complex theories ‚Üí more comparisons</p></li>
<li><p>Multidimensional data requires multiple views and summaries to fully understand</p></li>
<li><p>Harder to display all the data on a single static figure</p></li>
<li><p>Examining raw data becomes more difficult</p></li>
</ul>
</section>
<section id="section-2" class="slide level1" data-background="img/too-many-files.svg">
<h1></h1>
</section>
<section id="lupi" class="slide level1">
<h1></h1>
<blockquote>
‚Äú‚Ä¶to convey the richness of the data stories we are telling rather than simplifying them‚Äù
<footer>
‚Äî <a href="https://medium.com/accurat-studio/the-architecture-of-a-data-visualization-470b807799b4">Giorgia Lupi: The Architecture of a Data Visualization</a>
</footer>
</blockquote>
</section>
<section id="interactive-visualizations-can-help-us-quickly-make-comparisons-and-deal-with-complexity" class="slide level1" data-background="white">
<h1>Interactive visualizations can help us quickly make comparisons and deal with complexity</h1>
<figure>
<img src="img/heer-table.svg" alt="Heer &amp; Shneiderman (2012)" /><figcaption>Heer &amp; Shneiderman (2012)</figcaption>
</figure>
</section>
<section id="dynamic-visualizations-help-us-understand-complex-data-by-preserving-relationships-between-data" class="slide level1">
<h1>Dynamic visualizations help us understand complex data by preserving relationships between data</h1>
<figure>
<img src="img/Updating-Bar-Chart-720.gif" alt="Updating Bar Chart" /><figcaption>Updating Bar Chart</figcaption>
</figure>
</section>
<section id="web-enabled-visualizations-are-familiar-easily-shareable-and-enable-analysis-transparency" class="slide level1">
<h1>Web-enabled visualizations are familiar, easily shareable, and enable analysis transparency</h1>
<ul>
<li><p>Web-enabled visualizations work on many devices. No installation required.</p></li>
<li><p>Becoming more common in news graphics</p></li>
<li><p>Open Science</p></li>
</ul>
</section>
<section id="related-work" class="slide level1 two-figs">
<h1>Related Work</h1>
<figure>
<img src="img/GallantBrainViewer.gif" alt="pyCortex" /><figcaption><a href="https://github.com/gallantlab/pycortex">pyCortex</a></figcaption>
</figure>
<figure>
<img src="img/AllenBrainConnectivity.gif" alt="Allen Connectivity Atlas" /><figcaption><a href="http://connectivity.brain-map.org/">Allen Connectivity Atlas</a></figcaption>
</figure>
</section>
<section id="RasterVis" class="slide level1 githubRepo" data-background="img/RasterVisGithub.png">
<h1></h1>
<p><a href="/RasterVis/">Demo</a></p>
</section>
<section id="SpectraVis" class="slide level1 githubRepo" data-background="img/SpectraVisGithub.png">
<h1></h1>
<p><a href="/SpectraVis/public/">Demo</a></p>
</section>
<section id="glmVis" class="slide level1 githubRepo" data-background="img/glmVisGithub.png">
<h1></h1>
<p><a href="/glmVis/">Demo</a></p>
</section>
<section id="linking-visualizations-together-to-explore-datasets" class="slide level1">
<h1>Linking visualizations together to explore datasets</h1>
</section>
    </div>
  </div>

  <script src="revealjs/lib/js/head.min.js"></script>
  <script src="revealjs/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: false,
        // Display the page number of the current slide
        slideNumber: "c/t",
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'revealjs/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'revealjs/plugin/zoom-js/zoom.js', async: true },
          { src: 'revealjs/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
